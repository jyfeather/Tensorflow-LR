{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用DQN\n",
    "\n",
    "使用神经网络预测来替代查询Q table记录所有state action的奖励值。有两种思路：\n",
    "1. 输入state和action，通过神经网络，计算对应的Q值\n",
    "2. 输入state，通过神经网络，计算各个action的值。\n",
    "\n",
    "DQN有两个重要的技术：\n",
    "1. <span class=\"mark\">Experience replay</span>：能够抽取之前的经历进行学习\n",
    "2. <span class=\"mark\">Fixed Q-targets</span>：用于打乱经历之间的相关性，预测 Q 估计 的神经网络具备最新的参数, 而预测 Q 现实 的神经网络使用的参数则是很久以前的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN with experience replay\n",
    "初始化大小为 $N$ 的记忆库 $D$  \n",
    "使用随机权重 $\\theta$ 初始化行为-价值神经网络 $Q$  \n",
    "使用权重 $\\theta^{-}=\\theta$ 初始化目标的行为-价值函数 $\\hat{Q}$  \n",
    "循环从episode=1到$M$ \n",
    "> 初始化序列 $s_1=\\{x_1\\}$，以及预处理过的序列 $\\phi_1=\\phi(s_1)$  \n",
    "> 从t=1到$T$循环  \n",
    ">> 根据概率 $\\epsilon$，随机选择行为 $a_t$，或者选择 $a_t=\\arg\\max_a Q(\\phi(s_t),a;\\theta)$  \n",
    ">> 在模拟器中执行行为 $a_t$，并且观察奖励 $r_t$ 以及图像 $x_{t+1}$  \n",
    ">> 设置 $s_{t+1}=s_t,a_t,x_{t+1}$，并且预处理 $\\phi_{t+1}=\\phi(s_{t+1})$  \n",
    ">> 将序列 $(\\phi_t,a_t,r_t,\\phi_{t+1})$存入$D$  \n",
    ">> 从记忆库$D$中选择minibatch个序列 $(\\phi_j,a_j,r_j,\\phi_{j+1})$  \n",
    ">> 设置 $y_j$:  \n",
    ">>> 如果 episode 在 j+1 步终止，$y_j=r_j$  \n",
    ">>> 否则，$y_j = r_j+\\gamma\\max_{a'}\\hat{Q}(\\phi_{j+1},a';\\theta^{-})$\n",
    "\n",
    ">> 在 $(y_j - Q(\\phi_j,a_j;\\theta))^2$ 上执行梯度下降法，更新网络参数 $\\theta$  \n",
    ">> 每隔C步，更新 $\\hat{Q}=Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_env import Maze\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "确保可重复性\n",
    "\"\"\"\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    \"\"\"\n",
    "    off policy 的 DQN\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        n_features,\n",
    "        learning_rate = 0.01,\n",
    "        reward_decay = 0.9,\n",
    "        e_greedy = 0.9,\n",
    "        replace_target_iter = 300,\n",
    "        memory_size = 500,\n",
    "        batch_size = 32,\n",
    "        e_greedy_increment = None,\n",
    "        output_graph= False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "        \n",
    "        self.learn_step_counter = 0 # 总共的学习步数\n",
    "        self.memory = np.zeros((self.memory_size, n_features*2+2)) # 初始化空记忆体，[s,a,r,s_]\n",
    "        \n",
    "        # DQN 由 目标网络和预估网络组成，两个网络的结构相同，但参数大部分时候都不同\n",
    "        self._build_net()\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        self.replace_target_op = [tf.assign(t,e) for t,e in zip(t_params, e_params)] # 用于将预估网络参数复制给目标网络\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        if output_graph: # 使用 $ tensorboard --logdir=logs 打开\n",
    "            tf.summary.FileWriter('logs/', self.sess.graph)\n",
    "            \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.cost_his = []\n",
    "    \n",
    "    def _build_net(self):\n",
    "        \"\"\"\n",
    "        创建 eval 网络\n",
    "        \"\"\"\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s') # 用来接受状态 observation\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target') # 用来接受 q_target 的值，之后通过计算可以得到\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            \"\"\"\n",
    "            c_names (collections_names) 是在更新 target_net 参数的时候会用到\n",
    "            \"\"\"\n",
    "            c_names, n_l1, w_initializer, b_initializer = \\\n",
    "                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, \\\n",
    "                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1) \n",
    "            \n",
    "            \"\"\"\n",
    "            eval 网络的第一层，collections 是在更新 target_net 参数的时候才会用到\n",
    "            \"\"\"\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1)+b1)\n",
    "            \n",
    "            \"\"\"\n",
    "            eval 网络的第二层，collections 是在更新 target_net 参数的时候才会用到\n",
    "            \"\"\"\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_eval = tf.matmul(l1, w2)+b2\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        with tf.variable_scope('train'): # 梯度下降\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "        \n",
    "        \"\"\"\n",
    "        创建 target 网络， 提供 target Q\n",
    "        \"\"\"\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_') # 用来接受下一个状态 observation\n",
    "        with tf.variable_scope('target_net'):\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1) \n",
    "\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "        \n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        \n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        \n",
    "        # 替换记忆库中的内容\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index,:] = transition\n",
    "        \n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        observation = observation[np.newaxis, :]\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # 前向传递 observation， 得到每一个 action 对应的 Q 值\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        # 检查， 是否需要更新 target 网络\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            print('\\n target params replaced \\n')\n",
    "        \n",
    "        # 从记忆库中采样样本的batch\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "        \n",
    "        q_next, q_eval = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={\n",
    "                self.s_: batch_memory[:, -self.n_features:],\n",
    "                self.s: batch_memory[:, :self.n_features]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "        根据 eval 网络的 action， 改变 target 网络的结果\n",
    "        \"\"\"\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "        \n",
    "        # 训练 eval 网络\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                     feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                self.q_target: q_target})\n",
    "        self.cost_his.append(self.cost)\n",
    "\n",
    "        # increasing epsilon\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "    def plot_cost(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('training steps')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_maze():\n",
    "    step = 0 # 用来控制什么时候学习\n",
    "    for episode in range(300):\n",
    "        observation = env.reset() # 环境初始化\n",
    "        \n",
    "        while True:\n",
    "            env.render() # 刷新环境\n",
    "            action = RL.choose_action(observation) # DQN 根据观察值选择行为\n",
    "            observation_, reward, done = env.step(action) # 执行action\n",
    "            RL.store_transition(observation, action, reward, observation_) # DQN 存储记忆\n",
    "            \n",
    "            if (step > 200) and (step % 5 == 0): # 控制学习起始时间和频率（先积累一些记忆再开始学习）\n",
    "                RL.learn()\n",
    "                \n",
    "            observation = observation_ # 将下一个state_变为下次循环的state\n",
    "            \n",
    "            if done: # 如果终止，就跳出循环\n",
    "                break\n",
    "                \n",
    "            step += 1 # 记录总的步数\n",
    "    \n",
    "    # 游戏结束之后\n",
    "    print('game over')\n",
    "    env.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Maze()\n",
    "    RL = DeepQNetwork(\n",
    "        env.n_actions,\n",
    "        env.n_features,\n",
    "        learning_rate = 0.01,\n",
    "        reward_decay = 0.9,\n",
    "        e_greedy = 0.9,\n",
    "        replace_target_iter = 200, # 每隔200步，替换一次target_net的参数\n",
    "        memory_size = 2000, # 记忆上限\n",
    "        output_graph = True # 输出 tensorboard 文件\n",
    "    )\n",
    "    env.after(100, run_maze)\n",
    "    env.mainloop()\n",
    "    RL.plot_cost() # 观察神经网络的误差曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
