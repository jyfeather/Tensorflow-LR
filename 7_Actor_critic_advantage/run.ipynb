{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor 和 Critic\n",
    "\n",
    "强化学习中的一种结合体 Actor Critic (演员评判家), 它合并了 以值为基础 (比如 Q learning) 和 以动作概率为基础 (比如 Policy Gradients) 两类强化学习算法。\n",
    "\n",
    "1. Actor：Actor 的前生是 Policy Gradients, 这能让它毫不费力地在连续动作中选取合适的动作, 而 Q-learning 做这件事会瘫痪。\n",
    "2. Critic：Critic 的前生是 Q-learning 或者其他的 以值为基础的学习法 , 能进行单步更新, 而传统的 Policy Gradients 则是回合更新, 这降低了学习效率。\n",
    "\n",
    "Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率。\n",
    "\n",
    "Actor 修改行为时就像蒙着眼睛一直向前开车, Critic 就是那个扶方向盘改变 Actor 开车方向的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyjd/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "# Superparameters\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 3000\n",
    "DISPLAY_REWARD_THRESHOLD = 200  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 1000   # maximum time step in one episode\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_F = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor\n",
    "\n",
    "Parameterized policy: $P(a|s;w)$\n",
    "\n",
    "更新策略：\n",
    "$$\\Delta w = \\alpha \\delta(t) \\nabla P(a(t)|s(t);w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr=0.001):\n",
    "        \"\"\"\n",
    "        用 tensorflow 建立 Actor 神经网络,\n",
    "        搭建好训练的 Graph.\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.int32, None, \"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, \"td_error\")  # TD_error\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            l1 = tf.layers.dense( \n",
    "                inputs=self.s,\n",
    "                units=20,    # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.acts_prob = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=n_actions,    # output units\n",
    "                activation=tf.nn.softmax,   # get action probabilities\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='acts_prob'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('exp_v'):\n",
    "            log_prob = tf.log(self.acts_prob[0, self.a])\n",
    "            self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # advantage (TD_error) guided loss\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  # minimize(-exp_v) = maximize(exp_v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        \"\"\"\n",
    "        s, a 用于产生 Gradient ascent 的方向,\n",
    "        td 来自 Critic, 用于告诉 Actor 这方向对不对.\n",
    "        \"\"\"\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        \"\"\"\n",
    "        根据 s 选 行为 a\n",
    "        \"\"\"\n",
    "        s = s[np.newaxis, :]\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())   # return a int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.v_ = tf.placeholder(tf.float32, [1, 1], \"v_next\")\n",
    "        self.r = tf.placeholder(tf.float32, None, 'r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,  # number of hidden units\n",
    "                activation=tf.nn.relu,  # None\n",
    "                # have to be linear to make sure the convergence of actor.\n",
    "                # But linear approximator seems hardly learns the correct Q.\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.r + GAMMA * self.v_ - self.v\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        \"\"\"\n",
    "        学习 状态的价值 (state value), 不是行为的价值 (action value),\n",
    "        计算 TD_error = (r + v_) - v,\n",
    "        用 TD_error 评判这一步的行为有没有带来比平时更好的结果,\n",
    "        可以把它看做 Advantage\n",
    "        \"\"\"\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)\n",
    "critic = Critic(sess, n_features=N_F, lr=LR_C)     # we need a good teacher, so the teacher should learn faster than the actor\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        if done: r = -20\n",
    "\n",
    "        track_r.append(r)\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)     # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "\n",
    "        if done or t >= MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(track_r)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
