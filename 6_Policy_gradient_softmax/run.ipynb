{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 policy gradient\n",
    "\n",
    "Policy gradient 是 RL 中另外一个大家族, 他不像 Value-based 方法 (Q learning, Sarsa), 但他也要接受环境信息 (observation), 不同的是他要输出不是 action 的 value, 而是具体的那一个 action, 这样 policy gradient 就跳过了 value 这个阶段. 而且个人认为 Policy gradient 最大的一个优势是: 输出的这个 action 可以是一个连续的值, 之前我们说到的 value-based 方法输出的都是不连续的值, 然后再选择值最大的 action. 而 policy gradient 可以在一个连续分布上选取 action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient 之 Reinforce 算法\n",
    "我们介绍的 policy gradient 的第一个算法是一种基于 整条回合数据 的更新, 也叫 REINFORCE 方法. 这种方法是 policy gradient 的最基本方法。\n",
    "\n",
    "function Reinforce  \n",
    "> initialize $\\theta$ arbitrarily, where $\\pi_\\theta$ is the policy  \n",
    "> for each episode $\\{s_1,a_1,r_2,\\ldots,s_{T-1},a_{T-1},r_T\\} \\sim \\pi_\\theta$ do  \n",
    ">> for $t=1$ to $T-1$ do  \n",
    ">>> $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta (s_t,a_t)v_t$  \n",
    "\n",
    ">> end for  \n",
    "\n",
    "> end for\n",
    "> return \\theta\n",
    "\n",
    "end function  \n",
    "\n",
    "$\\log(Policy(s,a))\\cdot v$中的 $\\log(Policy(s,a))$表示状态 $s$对所选动作 $a$ 的吃惊度，如果 $Policy(s,a)$ 概率越小，反向的 $\\log(Policy(s,a))$ 越大。如果在$Policy(s,a)$很小的情况下，拿到了一个大的$R$，也就是$v$，那么$-\\log(Policy(s,a))\\cdot v$就更大，表示更吃惊，这就是$\\log(Policy)\\cdot v$的物理意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Discrete(2)\n",
      "Box(4,)\n",
      "4\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 可重复性\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "DISPLAY_REWARD_THRESHOLD = 400  # 在屏幕上显示模拟窗口会拖慢运行速度, 我们等计算机学得差不多了再显示模拟\n",
    "RENDER = False  # 在屏幕上显示模拟窗口会拖慢运行速度, 我们等计算机学得差不多了再显示模拟\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)     # 普通的 Policy gradient 方法, 使得回合的 variance 比较大, 所以我们选了一个好点的随机种子\n",
    "env = env.unwrapped\n",
    "\n",
    "print(env.action_space) # 显示可用 action\n",
    "print(env.observation_space) # 显示可用 state 的 observation\n",
    "print(env.observation_space.shape[0]) # feature (states) 的数量\n",
    "print(env.observation_space.high) # 显示 observation 最高值\n",
    "print(env.observation_space.low) # 显示 observation 最低值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.95,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            # http://0.0.0.0:6006/\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_net(self):\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.tf_obs = tf.placeholder(tf.float32, [None, self.n_features], name=\"observations\") # 各个时刻的状态，states\n",
    "            self.tf_acts = tf.placeholder(tf.int32, [None, ], name=\"actions_num\") # 允许采取的行为，action\n",
    "            self.tf_vt = tf.placeholder(tf.float32, [None, ], name=\"actions_value\") # 采取行为相应的奖励，reward\n",
    "        # fc1\n",
    "        layer = tf.layers.dense(\n",
    "            inputs=self.tf_obs,\n",
    "            units=10,\n",
    "            activation=tf.nn.tanh,  # tanh activation\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "            bias_initializer=tf.constant_initializer(0.1),\n",
    "            name='fc1'\n",
    "        )\n",
    "        # fc2\n",
    "        all_act = tf.layers.dense(\n",
    "            inputs=layer,\n",
    "            units=self.n_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "            bias_initializer=tf.constant_initializer(0.1),\n",
    "            name='fc2'\n",
    "        )\n",
    "\n",
    "        self.all_act_prob = tf.nn.softmax(all_act, name='act_prob')  # 输出每个action对应的概率\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            # to maximize total reward (log_p * R) is to minimize -(log_p * R), and the tf only have minimize(loss)\n",
    "            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act, labels=self.tf_acts)   # this is negative log of chosen action\n",
    "            # or in this way:\n",
    "            # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1)\n",
    "            loss = tf.reduce_mean(neg_log_prob * self.tf_vt)  # reward guided loss\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        prob_weights = self.sess.run(self.all_act_prob, feed_dict={self.tf_obs: observation[np.newaxis, :]})\n",
    "        action = np.random.choice(range(prob_weights.shape[1]), p=prob_weights.ravel())  # select action w.r.t the actions prob\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r):\n",
    "        self.ep_obs.append(s)\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "\n",
    "    def learn(self):\n",
    "        # discount and normalize episode reward\n",
    "        discounted_ep_rs_norm = self._discount_and_norm_rewards()\n",
    "\n",
    "        # train on episode\n",
    "        self.sess.run(self.train_op, feed_dict={\n",
    "             self.tf_obs: np.vstack(self.ep_obs),  # shape=[None, n_obs]\n",
    "             self.tf_acts: np.array(self.ep_as),  # shape=[None, ]\n",
    "             self.tf_vt: discounted_ep_rs_norm,  # shape=[None, ]\n",
    "        })\n",
    "\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []    # empty episode data\n",
    "        return discounted_ep_rs_norm\n",
    "\n",
    "    def _discount_and_norm_rewards(self):\n",
    "        # discount episode rewards\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(self.ep_rs))):\n",
    "            running_add = running_add * self.gamma + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "\n",
    "        # normalize episode rewards\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)\n",
    "        return discounted_ep_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义\n",
    "RL = PolicyGradient(\n",
    "    n_actions=env.action_space.n,\n",
    "    n_features=env.observation_space.shape[0],\n",
    "    learning_rate=0.02,\n",
    "    reward_decay=0.99, # gamma\n",
    "    output_graph=True, # 输出 tensorboard 文件\n",
    ")\n",
    "\n",
    "for i_episode in range(3000):\n",
    "    \"\"\"\n",
    "    让计算机跑完一整个回合才更新一次。 之前的 Q leanring 等在回合中每一步都可以更新参数。\n",
    "    \"\"\"\n",
    "\n",
    "    observation = env.reset()\n",
    "\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        action = RL.choose_action(observation) # 根据state，查询得到action\n",
    " \n",
    "        observation_, reward, done, info = env.step(action) # 执行action，返回新的state，以及reward\n",
    "\n",
    "        RL.store_transition(observation, action, reward) # 存储这一回合的 transition\n",
    "\n",
    "        if done:\n",
    "            \"\"\"\n",
    "            如果回合结束（也就是杆倒地了），则更新神经网络\n",
    "            \"\"\"\n",
    "            ep_rs_sum = sum(RL.ep_rs)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True     # 判断是否显示渲染\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "\n",
    "            vt = RL.learn() # 学习, 输出 vt, 我们下节课讲这个 vt 的作用\n",
    "\n",
    "            if i_episode == 0:\n",
    "                plt.plot(vt)    # plot 这个回合的 vt\n",
    "                plt.xlabel('episode steps')\n",
    "                plt.ylabel('normalized state-action value')\n",
    "                plt.show()\n",
    "            \n",
    "            observation = observation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
